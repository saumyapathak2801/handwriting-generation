{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "from mdn2 import *\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.graves_initializer = tf.truncated_normal_initializer(mean=0., stddev=.075, seed=None, dtype=tf.float32)\n",
    "        self.window_b_initializer = tf.truncated_normal_initializer(mean=-3.0, stddev=.25, seed=None, dtype=tf.float32)\n",
    "        self.learning_rate = 0.001\n",
    "        self.tsteps = args['tsteps']\n",
    "        self.num_mixtures = args['num_mixtures']\n",
    "        self.rnn_size = args['rnn_size']\n",
    "        self.batch_size = args['batch_size']\n",
    "        self.biases = args['biases']\n",
    "        self.grad_clip = args['grad_clip']\n",
    "        \n",
    "        # Build an LSTM cell, each cell has rnn_size number of units\n",
    "        with tf.variable_scope(tf.get_variable_scope(),reuse=False):\n",
    "            cell_func = tf.contrib.rnn.LSTMCell\n",
    "            self.cell0 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.graves_initializer)\n",
    "            self.cell1 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.graves_initializer)\n",
    "            self.cell2 = cell_func(args['rnn_size'], state_is_tuple=True, initializer=self.graves_initializer)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Placeholders for input and output data, each entry has tsteps points at a time\n",
    "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, args['tsteps'], 3])\n",
    "        self.output = tf.placeholder(dtype=tf.float32, shape=[None, args['tsteps'], 3])\n",
    "        \n",
    "        # Setting the states of memory cells in each LSTM cell.\n",
    "        # batch_size is the number of training examples in a batch. Each training example is a set of tsteps number of\n",
    "        # (x,y, <end_of_stroke>) tuples, i.e. a sequence of strokes till t time steps.\n",
    "        self.istate_cell0 = self.cell0.zero_state(batch_size=args['batch_size'], dtype=tf.float32)\n",
    "        self.istate_cell1 = self.cell1.zero_state(batch_size=args['batch_size'], dtype=tf.float32)\n",
    "        self.istate_cell2 = self.cell2.zero_state(batch_size=args['batch_size'], dtype=tf.float32)\n",
    "        \n",
    "        # Input to model is a set of batch_size number of training samples. Step below splits by tsteps, giving one element in a tstep worth in each batch\n",
    "        input_to_model = [tf.squeeze(input_, [1]) for input_ in tf.split(self.input, self.tsteps, 1)]\n",
    "        \n",
    "        def build_computational_graph(self, inputs, cell, initial_cell_state, scope):\n",
    "            output, cell_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(inputs, initial_cell_state, cell, loop_function=None, scope=scope)\n",
    "            return [output, cell_final_state]\n",
    "        \n",
    "#         with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "#             outs_layer0, self.cell0_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(input_to_model, self.istate_cell0, self.cell0, loop_function=None, 'cell0')\n",
    "#             outs_layer1, self.cell1_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(outs_layer0, self.istate_cell1, self.cell1, loop_function=None, 'cell1')\n",
    "#             outs_layer2, self.cell2_final_state = tf.contrib.legacy_seq2seq.rnn_decoder(outs_layer1, self.istate_cell2, self.cell2, loop_function=None, 'cell2')\n",
    "        outs_layer0, self.cell0_final_state = build_computational_graph(self, input_to_model, self.cell0, self.istate_cell0, 'cell0')\n",
    "        outs_layer1, self.cell1_final_state = build_computational_graph(self, outs_layer0, self.cell1, self.istate_cell1, 'cell1')\n",
    "        outs_layer2, self.cell2_final_state = build_computational_graph(self, outs_layer1, self.cell2, self.istate_cell2, 'cell2')\n",
    "        \n",
    "        # The output of final layer goes into MDN\n",
    "        # for each output we predict 6 parameters + 1 eos for \"nmixture\" mixture density components\n",
    "        total_mdn_params = 6 * self.num_mixtures + 1\n",
    "        \n",
    "        # according to eq(17) in https://arxiv.org/pdf/1308.0850.pdf\n",
    "        with tf.variable_scope('mdn_dense'):\n",
    "            # initializing W and b matrix for eq(17)\n",
    "            W_to_mdn = tf.get_variable(\"output_w\", [self.rnn_size, total_mdn_params], initializer=self.graves_initializer)\n",
    "            b_to_mdn = tf.get_variable(\"output_b\", [total_mdn_params], initializer=self.graves_initializer)\n",
    "            \n",
    "        outs_layer2 = tf.reshape(tf.concat(outs_layer2, 1), [-1, args['rnn_size']])\n",
    "        output_layer = tf.nn.xw_plus_b(outs_layer2, W_to_mdn, b_to_mdn) #eq(17) in https://arxiv.org/pdf/1308.0850.pdf\n",
    "        flat_output = tf.reshape(self.output,[-1, 3])\n",
    "        [output_x, output_y, eos_data] = tf.split(flat_output, 3, 1)\n",
    "        \n",
    "        # MDN\n",
    "        [self.eos, self.pi, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho] = get_mdn_coef(self, output_layer)\n",
    "        loss = get_loss(self.pi, output_x, output_y, eos_data, self.mu1, self.mu2, self.sigma1, self.sigma2, self.rho, self.eos)\n",
    "        self.cost = loss / (self.batch_size * self.tsteps) # J = 1/m*sum(Loss) , m = number of training examples\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        \n",
    "        self.learning_rate = tf.Variable(0.0, trainable=False)\n",
    "        self.decay = tf.Variable(0.0, trainable=False)\n",
    "        self.momentum = tf.Variable(0.0, trainable=False)\n",
    "        #self.optimizer = tf.train.RMSPropOptimizer(learning_rate=self.learning_rate, decay=self.decay, momentum=self.momentum)\n",
    "        \n",
    "#         tvars = tf.trainable_variables()\n",
    "#         grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tvars), self.grad_clip)\n",
    "#         self.train_op = self.optimizer.apply_gradients(zip(grads, tvars))\n",
    "        self.train_op = self.optimizer.minimize(self.cost)\n",
    "\n",
    "    \n",
    "    \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
